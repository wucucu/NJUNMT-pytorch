# 分布式训练

**注意:** 目前我们我们只测试了单机多卡的正确性。

<!-- toc -->

- [使用方法](#)
- [Q&A](#QA)
  * [为什么我们不使用`DistributedDataParallel`](#DistributedDataParallel)
  * [为什么我们不使用`Horovod`](#Horovod)

<!-- tocstop -->

## 使用方法

我们采取一个launch脚本来启动分布式训练。比如，我们希望在一台具有四块GPU的服务器上进行训练，那么如下修改训练的脚本:

``` bash
python -m src.distributed.launch \
    --nproc_per_node=4 \
    --nnodes=1 --node_rank=0 \
    --master_addr="127.0.0.1" --master_port=1234 \
    src.bin.train \
    --model_name <your-model-name> \
    --reload \
    --config_path <your-config-path> \
    --log_path <your-log-path> \
    --saveto <path-to-save-checkpoints> \
    --valid_path <path-to-save-validation-translation> \
    --use_gpu \
    --shared_dir "/tmp"
```

其中，`nnodes`表明每个总共有一个节点(机器)，`nproc_per_node`表明这个节点上有4个进程(四块GPU，因为我们对于每块GPU的训练任务分配一个进程)，`node_rank`表明该节点的顺序在整个任务中属于0号节点(从0开始编号，因为是单机，所以自然是0)；`master_addr`和`master_port`分别表明主节点的ip地址和端口号，同样因为单机，所以ip地址设为localhost即可；`shared_dir`指定一个共享的路径，用于在进程之前传递一些较大的数据，具体方式为:

    1.将每个进程需要共享的数据在这个共享路径下保存为一些临时文件。
    2.在进程之间传递这些临时文件的路劲。
    3.读取，合并并还原出需要共享的数据

按照类似的方式，我们同样可以运行分布式解码。

## Q&A

### 为什么我们不使用`DistributedDataParallel`

`DistributedDataParallel`(DDP)目前通过hook的方式来进程间梯度的通讯，即每一次求导后，DDP会立即开始执行allreduce操作，向其他交换梯度的信息。这是一种常见的合拼计算开销和通讯开销的技巧。然而，这样的实现没办法兼容`update_cycle`机制，即在将较大的batch切分成若干个小batch计算并累计梯度。此外，需要将程序中的每一个module转换成DDP类型的module，个人认为也较为不便。

为此，我们参考了Horovod的设计，将这些操作放在了optimizer的位置，并改为在执行梯度更新之前手动执行梯度的allreduce操作。这样我们可以执行module若干forward和backward操作，然后由optimizer完成参数更新。当然，这样会造成一些性能上的损失。

### 为什么我们不使用`Horovod`

使用Horovod需要安装OpenMPI，其安装时较为繁琐的(尤其在没有sudo权限的情况下)。此外，Horovod并不支持在windows上安装。

